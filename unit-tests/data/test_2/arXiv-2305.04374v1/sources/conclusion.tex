\section{Conclusion}
\label{sec:conclusion}

\paragraph{Limitations and future work} Our current method requires camera poses and depth maps as inputs. For single-view lighting prediction, we can achieve state-of-the-art performances with depth prediction from \cite{niklaus20193d}. For video lighting prediction, our method works well with the camera poses and low-resolution depth maps captured by iPad Arkit. How to predict temporally consistent depth maps is still an open problem \cite{luo2020consistent,zhang2021consistent} and even depth maps captured by ARKit flicker noticeably. However, as shown in several real examples in Figure \ref{fig:teaser}, Figure \ref{fig:rnn} and Figure \ref{fig:videoReal}, our method is robust to such flickering. In addition, if the input video sequence is too long, the camera poses from ARKit will be less accurate and cause errors in our lighting prediction. As higher quality sensors become more and more accessible on mobile devices, we suggest that these limitations may be less influential in the future. 

In addition, our method may fail in the presence of highly glossy or mirror materials. In Figure \ref{fig:rnn}, we can see that the reflection of highlight on the tablet gets blurred in our HDR environment map prediction as the camera moves away. This is because as we change our viewpoints, this highlight is no longer visible in later frames (Frame 13, for example). This suggests that we need a holistic framework to jointly reason about the geometry and materials of indoor scenes to achieve more accurate lighting prediction. 

Besides, our method cannot hallucinate detailed reflection from invisible surfaces. We argue that this is a too challenging problem and humans may not be very sensitive to these details. Prior method \cite{srinivasan2020lighthouse} tries to solve this problem by adding adversarial loss. However, as shown in both synthetic and real examples in Figure \ref{fig:videoSyn} and Figure \ref{fig:videoReal}, they only add random textures to the environment map prediction without substantially improving its visual quality. We also tried adversarial loss but observe that it deteriorates our model's generalization ability by adding weird colors in some real cases. On the contrary, our current method generally can predict the overall color distribution of invisible reflection with smooth transition between visible and invisible regions, leading to more visually convincing lighting prediction. How to synthesize realistic consistent invisible details can be an interesting direction for future exploration.

Finally, our current method is designed for indoor scenes only and will not generalize well to outdoor scenes. While our lighting representation can model directional lighting, our training data contain only indoor scene images. Moreover, the scales of outdoor scenes are much larger than indoor scenes, which will require us to modify our volume parameterization to handle scene structures that are far away from the camera. In addition, our video lighting prediction method can only handle static scenes but cannot handle dynamic scenes with changing lighting or moving objects.

\paragraph{Summary} We present a physically-motivated deep learning-based framework for spatiotemporally consistent HDR lighting prediction at arbitrary locations of indoor scenes, which enables realistic object insertion with different materials. Our method is the first that can utilize video sequences to improve lighting prediction, while explicitly considering temporal consistency as we progressively refine the outputs. The key novelties of our framework include: (1) a hybrid framework that can recover both visible details and HDR light sources as well as guarantee spatial consistency and (2) an RNN-based framework that can handle video sequences. Experiments show that our framework is more general, as it can handle more diverse inputs for different applications, and with comparable performances to prior state-of-the-arts. 