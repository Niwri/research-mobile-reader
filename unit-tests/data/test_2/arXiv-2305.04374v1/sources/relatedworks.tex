\section{Related Works}
\label{sec:relatedworks}

Indoor lighting estimation is a long-standing challenge in computer vision and graphics and is of fundamental importance to achieving photorealistic augmented reality. The pioneering work by Debevec captures indoor lighting by taking multiple images of a mirror sphere with different exposure times \cite{debevec1998rendering}.  While it can capture high-quality detailed HDR environment maps, it needs careful calibration, which is not friendly to non-expert users and cannot model spatially-varying lighting effects. Several model-based methods formulate indoor lighting estimation as part of a holistic inverse rendering framework and jointly reason about geometry, reflectance and lighting by minimizing hand-crafted energy functions. Notably, Barron and Malik \cite{barron2013intrinsic} model the indoor scene appearance with a diffuse reflectance map and a mixture of geometry and lighting basis driven from data priors, which allows them to simultaneously recover diffuse reflectance, refined depth map and per-pixel lighting from a single RGBD image. Karsch et al. \cite{karsch-tog-14} approximate indoor lighting by selecting a HDR panorama from a large-scale dataset. Then, they optimize the light source position and intensity by solving an inverse rendering problem based on estimated diffuse reflectance and depth map. These methods rely on certain assumptions to simplify the problem and therefore may not achieve high-quality lighting estimation in complex scenes, with the presence of complex directional lighting, strong shadows caused by large occlusion and highly glossy materials. 

Recent methods apply deep learning frameworks to tackle this highly ill-posed classical challenge. However, as summarized in Table \ref{tab:summary}, none of them can achieve all the desirable properties for indoor lighting estimation. Akimoto et al. \cite{akimoto2022diverse} and Dastjerdi et al. \cite{dastjerdi2022guided} extrapolate a 360 degree field of view panorama from a single image at the center of it using a deep generative network. Gardner et al. \cite{gardner2017indoor}, Sengupta et al. \cite{sengupta2019neural}, Zhan et al. \cite{zhan2021emlight}, and LeGendre et al. \cite{legendre2019deeplight} predict a single lighting for the whole indoor scene. These methods cannot model important spatially-varying lighting. Several methods can predict spatially-varying lighting from a single indoor image. Particularly, Garon et al. \cite{garon2019fast} and Zhao et al. \cite{zhao2020pointar} use spherical harmonics bases \cite{ramamoorthi2001efficient} to approximate indoor HDR environment maps and therefore can only recover low frequency signals. Li et al. \cite{li2020inverse}, Zhu et al. \cite{zhu2022irisformer}, and Li et al. \cite{li2022physically} use spherical Gaussian lobes while Zhan et al. \cite{zhan2021sparse} use wavelets instead to better preserve high-frequency directional lighting. However, neither of them can recover the near-field details in an environment map, which is important for rendering realistic reflectance on mirror and highly glossy surfaces. Song et al. \cite{Song_2019_CVPR}, Somanath et al. \cite{somanath2021hdr}, and Zhan et al. \cite{zhan2022gmlight} directly predict the 2D environment map by inpainting the partial observation from the input image through generative modeling. Nevertheless, one important limitation of all the above spatially varying lighting prediction methods is that they cannot guarantee spatial consistency. This may cause flickering artifacts if we use their predicted lighting to render a virtual object moving around the scene. In contrast, Gardner et al. \cite{gardner2019parametric} and Srinivasan et al. \cite{srinivasan2020lighthouse} can predict spatially consistent lighting at any positions,  by reconstructing environment maps from their 3D scene representations, i.e. spherical Gaussian light sources and RGB$\alpha$ volume, through physics-based ray tracing. However, Gardner et al. cannot handle occlusion, while Srinivasan et al. cannot handle HDR and directional lighting. A very recent work \cite{wang2021learning} by Wang et al. solves the limitation of \cite{srinivasan2020lighthouse} by introducing spherical Gaussian volume to handle directional lighting, which is similar to our proposal. However, both of them use high-resolution volumes to recover detailed reflection while our hybrid representation achieves better details with much less computational cost. More importantly, none of them can handle video inputs with temporal consistency.

The only prior method that considers video inputs is proposed by Wei et al. \cite{wei2020object}. However, their method is not demonstrated to be able to use incoming frames to progressively improve the lighting prediction. On the contrary, our method is the first learning-based indoor lighting prediction framework that can enhance lighting prediction with video input, while preserving both spatial and temporal consistency. Our method achieves all the desirable properties for indoor lighting prediction, which allows us to obtain a higher level of photorealism in various AR applications with better generalization. 

